# -*- coding: utf-8 -*-
"""
Elon Musk æ¨æ–‡æ•°æ®æ¸…æ´— + åˆ†æ å·¥å…·ï¼ˆäº¤äº’èœå•ç‰ˆï¼‰
- 1 æ¸…æ´—ï¼šå°†æ··ä¹±çš„ XTracker å¯¼å‡º CSV åˆå¹¶ç¢è¡Œã€ä¿®å¤å¼•å·ã€æ¨æ–­å¹´ä»½ã€ç”ŸæˆåŒ—äº¬æ—¶é—´åˆ—
- 2 åˆ†æï¼šåŸºç¡€æ¦‚è§ˆï¼ˆæœˆä»½/æ˜ŸæœŸ/å°æ—¶ï¼‰
- 3 é«˜çº§åˆ†æï¼šæŒ‰ä½ çš„ç»´åº¦éœ€æ±‚ï¼ˆå‘¨ç»´åº¦ã€å°æ—¶ç»´åº¦ã€å‘¨+å°æ—¶ç»´åº¦ï¼‰ï¼Œæ”¯æŒè¾“å…¥â€œè¿‡å» N ä¸ªæœˆâ€
"""

import os
import re
import glob
import calendar
from pathlib import Path
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Tuple, Dict, Set

from rich.console import Console
from rich.table import Table
from rich.prompt import Prompt, IntPrompt
from rich.panel import Panel
from rich.progress import track

console = Console()
BASE_DIR = Path(__file__).resolve().parent

# ====================== é…ç½® ======================
INPUT_FILE = "elonmusk.csv"            # åŸå§‹å¯¼å‡ºæ–‡ä»¶ï¼ˆæœªæ¸…æ´—ï¼‰
OUTPUT_PREFIX = str(BASE_DIR / "elonmusk_clean")  # æ¸…æ´—æ–‡ä»¶å‰ç¼€ï¼ˆç»å¯¹è·¯å¾„ï¼‰
ENCODING = "utf-8"                     # è¯»å–åŸå§‹æ–‡ä»¶æ—¶çš„ç¼–ç 
START_YEAR = 2024                      # ç¬¬ä¸€æ®µæœˆä»½æ‰€å±å¹´ä»½ï¼ˆåç»­é‡åˆ°æœˆä»½å›å·åˆ™ +1 å¹´ï¼‰
MONTH_ORDER_ENG = ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]
WEEK_ORDER = ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]
# ==================================================


# ------------------------ å·¥å…·å‡½æ•° ------------------------
def next_output_name(prefix: str) -> str:
    """æ‰«æç°æœ‰ clean æ–‡ä»¶å¹¶ç”Ÿæˆä¸‹ä¸€ä¸ªä¸‰ä½ç¼–å·çš„è¾“å‡ºæ–‡ä»¶åã€‚"""
    exists = sorted(glob.glob(f"{prefix}_*.csv"))
    if not exists:
        return f"{prefix}_001.csv"
    nums = []
    for f in exists:
        m = re.search(r'_(\d{3})\.csv$', f)
        if m:
            nums.append(int(m.group(1)))
    return f"{prefix}_{(max(nums)+1 if nums else 1):03d}.csv"


def parse_est_datetime(series: pd.Series) -> pd.Series:
    """
    å°† â€œYYYY-MM-DD HH:MM:SS EDT/ESTâ€ è§£æä¸ºå¸¦æ—¶åŒºçš„ America/New_York æ—¶é—´ã€‚
    """
    if series.empty:
        return pd.Series(dtype="datetime64[ns, America/New_York]")
    normalized = (
        series.astype(str)
        .str.strip()
        .str.replace(" EDT", " -0400", regex=False)
        .str.replace(" EST", " -0500", regex=False)
    )
    dt = pd.to_datetime(normalized, format="%Y-%m-%d %H:%M:%S %z", errors="coerce", utc=True)
    return dt.dt.tz_convert("America/New_York")


def parse_bj_datetime(series: pd.Series) -> pd.Series:
    """
    å°† â€œYYYY-MM-DD HH:MM:SS CSTâ€ è§£æä¸º Asia/Shanghai æ—¶é—´ã€‚
    """
    normalized = series.astype(str).str.strip().str.replace(" CST", " +0800", regex=False)
    dt = pd.to_datetime(normalized, format="%Y-%m-%d %H:%M:%S %z", errors="coerce", utc=True)
    return dt.dt.tz_convert("Asia/Shanghai")


def coalesce_records(lines: List[str]) -> List[str]:
    """æŠŠéä»¥æ¨æ–‡IDå¼€å¤´çš„è¡Œå¹¶å›ä¸Šä¸€æ¡ï¼Œä¿è¯ä¸€æ¡æ¨æ–‡åªå ä¸€è¡Œï¼ˆç²—åˆå¹¶ï¼‰"""
    recs, buf = [], []
    id_head_re = re.compile(r'^\s*"?(\d{18,19})"?,')
    for raw in lines:
        line = raw.rstrip("\n\r")
        if id_head_re.match(line):
            if buf:
                recs.append(" ".join(buf).strip())
                buf = []
            buf.append(line.strip())
        else:
            buf.append(line.strip())
    if buf:
        recs.append(" ".join(buf).strip())
    return recs


def clean_content_text(s: str) -> str:
    """æ¸…æ´—å†…å®¹å­—æ®µé‡Œçš„å¤šä½™å¼•å·/é€—å·æ–­è£‚"""
    # å…ˆæŠŠ '","' çš„ CSV åˆ†éš”æƒ…å†µæ¸©å’Œå¤„ç†ä¸ºé€—å·+ç©ºæ ¼ï¼ˆä¸ç ´åæœ€å¤–å±‚ï¼‰
    s = s.replace('","', ', ')
    # å»æ‰æœ€å¤–å›´å¼•å·
    if len(s) >= 2 and s[0] == '"' and s[-1] == '"':
        s = s[1:-1]
    # å†…éƒ¨åŒå¼•å·è½¬ä¹‰è¿˜åŸ
    s = s.replace('""', '"')
    # å‹ç¼©å¤šç©ºç™½
    s = re.sub(r'\s+', ' ', s)
    return s.strip()


def parse_record(rec: str):
    """
    ä»åˆå¹¶åçš„å•è¡Œè®°å½•é‡ŒæŠ½å–ï¼š
    - ID
    - å†…å®¹
    - æ—¥æœŸç¢ç‰‡ï¼ˆMon Day, hh:mm:ss AM/PM TZï¼‰
    """
    # æŠ“æœ€å³ä¾§çš„æ—¥æœŸæ—¶é—´ç‰‡æ®µ
    tail_re = re.compile(
        r',\s*("?(?P<mon>[A-Za-z]{3})\s+(?P<day>\d{1,2}),\s+(?P<time>\d{1,2}:\d{2}:\d{2})\s+(?P<ampm>AM|PM)\s+(?P<tz>[A-Z]{2,4})"?)\s*$'
    )
    m_date = tail_re.search(rec)
    if not m_date:
        return None

    m_id = re.match(r'^\s*"?(\d{18,19})"?,', rec)
    if not m_id:
        return None
    tw_id = m_id.group(1)

    first_comma_idx = rec.find(",", m_id.end() - 1)
    if first_comma_idx == -1:
        return None

    date_span_start = m_date.start()
    content_chunk = rec[first_comma_idx + 1 : date_span_start]
    content_clean = clean_content_text(content_chunk)

    return tw_id, content_clean, m_date.groupdict()


def assign_years(parsed_records: List[Tuple[str, str, Dict[str, str]]]) -> Tuple[List[Tuple[str,str,str,str,int,int,str,int]], Set[int]]:
    """
    æ ¹æ®æœˆä»½å›å·è§„åˆ™ä¸ºæ¯æ¡è®°å½•åˆ†é…å¹´ä»½ï¼Œå¹¶ç”Ÿæˆâ€œEDTæ—¶é—´å­—ç¬¦ä¸² + åŒ—äº¬æ—¶é—´å­—ç¬¦ä¸²â€
    è¿”å›ï¼š
      rows: [(id, content, EDTæ—¶é—´å­—ç¬¦ä¸², åŒ—äº¬æ—¶é—´å­—ç¬¦ä¸², å¹´ä»½, æœˆä»½, å‘¨å‡ ç®€ç§°, å°æ—¶)]
      all_years: æ¶‰åŠåˆ°çš„å¹´ä»½é›†åˆ
    """
    year = START_YEAR
    month_idx_prev = None
    results = []
    all_years = set()

    for rid, content, date_info in parsed_records:
        mon = date_info["mon"]
        if mon not in MONTH_ORDER_ENG:
            continue
        idx = MONTH_ORDER_ENG.index(mon)  # 0-11

        if month_idx_prev is not None and idx < month_idx_prev:
            year += 1
            console.log(f"ğŸ“† æ£€æµ‹åˆ°è·¨å¹´ï¼š{MONTH_ORDER_ENG[month_idx_prev]}â†’{mon}ï¼Œå¹´ä»½åˆ‡æ¢åˆ° {year}")
        month_idx_prev = idx
        all_years.add(year)

        day = int(date_info["day"])
        time_str = date_info["time"]
        ampm = date_info["ampm"]
        tz = date_info["tz"]

        # è§£æ 12 å°æ—¶åˆ¶
        t = datetime.strptime(f"{time_str} {ampm}", "%I:%M:%S %p")
        edt_dt = datetime(year, idx + 1, day, t.hour, t.minute, t.second)
        # æ ¹æ®åŸå§‹æ ‡è¯†åˆ¤æ–­æ—¶å·®ï¼šEDT(+12h)ã€EST(+13h)ï¼Œé»˜è®¤æŒ‰ 12 å°æ—¶å¤„ç†
        tz_upper = tz.upper()
        offset_hours = 13 if tz_upper == "EST" else 12
        bj_dt = edt_dt + timedelta(hours=offset_hours)

        edt_fmt = f"{year:04d}-{idx+1:02d}-{day:02d} {t.strftime('%H:%M:%S')} {tz}"
        bj_fmt  = bj_dt.strftime("%Y-%m-%d %H:%M:%S CST")

        extra_year = bj_dt.year
        extra_month = bj_dt.month
        extra_weekday = bj_dt.strftime("%a")
        extra_hour = bj_dt.hour

        results.append((rid, content, edt_fmt, bj_fmt, extra_year, extra_month, extra_weekday, extra_hour))

    return results, all_years


def robust_parse_bj(series: pd.Series) -> pd.Series:
    """
    æ›´é²æ£’åœ°è§£æ â€œåŒ—äº¬æ—¶é—´â€ åˆ—ä¸º pandas datetimeï¼š
    - å»æ‰æœ«å°¾æ—¶åŒºå­—æ ·ï¼ˆå¦‚ CSTï¼‰
    - å…ˆå°è¯•å›ºå®šæ ¼å¼ï¼Œå† fallback åˆ° dateutil
    """
    s = series.astype(str).str.replace(r'\s+[A-Z]{2,4}$', '', regex=True)
    dt = pd.to_datetime(s, format="%Y-%m-%d %H:%M:%S", errors="coerce")
    if dt.isna().all():
        dt = pd.to_datetime(s, errors="coerce")
    return dt


def filter_last_n_months(df: pd.DataFrame, n_months: int) -> pd.DataFrame:
    """
    ä»¥æ•°æ®ä¸­â€œåŒ—äº¬æ—¶é—´â€çš„æœ€å¤§æœˆä»½ä¸ºé”šç‚¹ï¼Œå›æº¯ N ä¸ªæœˆï¼ˆåŒ…å«é”šç‚¹æœˆï¼‰ï¼Œè¿”å›è¯¥åŒºé—´æ•°æ®
    """
    dt = robust_parse_bj(df["åŒ—äº¬æ—¶é—´"])
    df = df.copy()
    df["__dt"] = dt
    max_dt = df["__dt"].max()
    if pd.isna(max_dt):
        return df.iloc[0:0]
    # è®¡ç®—èµ·ç‚¹ï¼ˆæ»šåŠ¨ n-1 ä¸ªæœˆï¼‰
    year, month = max_dt.year, max_dt.month
    for _ in range(n_months - 1):
        month -= 1
        if month == 0:
            month = 12
            year -= 1
    start_dt = datetime(year, month, 1)
    # ç»ˆç‚¹ä¸ºé”šç‚¹æœˆæœ€åä¸€å¤© 23:59:59
    end_day = calendar.monthrange(max_dt.year, max_dt.month)[1]
    end_dt = datetime(max_dt.year, max_dt.month, end_day, 23, 59, 59)

    mask = (df["__dt"] >= start_dt) & (df["__dt"] <= end_dt)
    return df.loc[mask].drop(columns=["__dt"])


def build_day_bucket_stats(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç”Ÿæˆâ€œ12:00 â†’ æ¬¡æ—¥ 12:00â€çš„æŒ‰å¤©ç»Ÿè®¡ã€‚é€šè¿‡æŠŠæ—¶é—´æ•´ä½“å‡å» 12 å°æ—¶ï¼Œ
    å†æŒ‰æ—¥æœŸåˆ†ç»„å³å¯å¾—åˆ°ä»¥ä¸­åˆä¸ºç•Œçš„çª—å£ã€‚
    """
    est_series = parse_est_datetime(df["EDT_time"])
    shifted = (est_series - pd.to_timedelta(12, unit="h")).dt.tz_localize(None).dropna()
    day_counts = shifted.dt.floor("D").value_counts().sort_index()
    result = day_counts.reset_index()
    result.columns = ["date", "day_tweet_count"]
    result["week_day"] = pd.to_datetime(result["date"]).dt.strftime("%a")
    result["date"] = pd.to_datetime(result["date"]).dt.strftime("%m/%d/%Y")
    return result[["date", "week_day", "day_tweet_count"]]


def build_natural_day_stats(df: pd.DataFrame) -> pd.DataFrame:
    """æŒ‰è‡ªç„¶æ—¥ï¼ˆEST 0:00-23:59ï¼‰ç»Ÿè®¡æ¨æ–‡æ•°é‡ï¼Œç”¨äºæ—¥çº¿å±•ç¤ºã€‚"""
    est_series = parse_est_datetime(df["EDT_time"])
    natural = (
        pd.DataFrame({"date": est_series.dt.tz_localize(None).dt.date, "week_day": est_series.dt.strftime("%a")})
        .dropna()
    )
    counts = (
        natural.groupby(["date", "week_day"]).size().reset_index(name="day_tweet_count")
    )
    counts["date"] = pd.to_datetime(counts["date"]).dt.strftime("%m/%d/%Y")
    return counts[["date", "week_day", "day_tweet_count"]]


def build_hourly_stats(df: pd.DataFrame) -> pd.DataFrame:
    """ç”Ÿæˆé€å°æ—¶æ¨æ–‡ç»Ÿè®¡ï¼ŒåŒæ—¶è®°å½•ç¾ä¸œä¸åŒ—äº¬æ—¶é—´çš„å°æ—¶ã€‚"""
    est_series = parse_est_datetime(df["EDT_time"])
    bj_series = parse_bj_datetime(df["Beijing_time"])
    valid_mask = est_series.notna() & bj_series.notna()
    est_series = est_series[valid_mask]
    bj_series = bj_series[valid_mask]
    temp = pd.DataFrame(
        {
            "date": est_series.dt.tz_convert(None).dt.strftime("%m/%d/%Y"),
            "week_day": est_series.dt.strftime("%a"),
            "hour_us": est_series.dt.hour,
            "hour_cn": bj_series.dt.hour,
        }
    )
    counts = (
        temp.value_counts()
        .reset_index(name="hour_tweet_count")
        .sort_values(["date", "hour_us", "hour_cn"])
    )
    return counts


# ------------------------ 1) æ¸…æ´— ------------------------
def run_cleaning():
    """
    è¯»å–åŸå§‹ XTracker CSVï¼Œé‡å»ºç ´ç¢è¡Œå¹¶è¡¥å…¨æ—¶é—´ä¿¡æ¯ï¼Œäº§å‡ºæ ‡å‡†åŒ–çš„ clean æ–‡ä»¶ã€‚
    æ­¥éª¤ï¼š
      1. åˆå¹¶ç¢è¡Œå¹¶è§£æ tweet_id / å†…å®¹ / æ—¥æœŸç‰‡æ®µï¼›
      2. æ ¹æ®æœˆä»½å›å·æ¨æ–­å¹´ä»½å¹¶ç”ŸæˆåŒ—äº¬æ—¶é—´åˆ—ï¼›
      3. å†™å…¥æ–°çš„ CSV å¹¶æ‰“å°æ¸…æ´—æ‘˜è¦ã€‚
    """
    if not os.path.exists(INPUT_FILE):
        console.print(f"[red]âŒ æœªæ‰¾åˆ°æºæ–‡ä»¶ï¼š{INPUT_FILE}[/red]")
        return

    out_name = next_output_name(OUTPUT_PREFIX)

    console.print(f"\n[bold cyan]ğŸš€ å¼€å§‹æ¸…æ´—æ–‡ä»¶ï¼š[/bold cyan]{INPUT_FILE}\n")
    with open(INPUT_FILE, "r", encoding=ENCODING, errors="ignore") as f:
        raw_lines = f.readlines()
    total_lines = len(raw_lines)
    if total_lines == 0:
        console.print("[red]âŒ æ–‡ä»¶ä¸ºç©º[/red]")
        return

    header = raw_lines[0].strip()
    coalesced = coalesce_records(raw_lines[1:])
    # ç»Ÿè®¡åŸå§‹æœ‰æ•ˆè®°å½•ï¼ˆç²—ä¼°ï¼šä»¥ id å¼€å¤´çš„è¡Œæ•°ï¼‰
    raw_valid = sum(1 for line in raw_lines[1:] if re.match(r'^\s*"?\d{18,19}"?,', line))

    parsed_records = []
    for rec in coalesced:
        pr = parse_record(rec)
        if pr:
            parsed_records.append(pr)

    rows, all_years = assign_years(parsed_records)
    df_clean = pd.DataFrame(
        [
            (content, edt_str, bj_time, year_val, month_val, weekday_val, hour_val)
            for _tw_id, content, edt_str, bj_time, year_val, month_val, weekday_val, hour_val in rows
        ],
        columns=["content", "EDT_time", "Beijing_time", "year", "Month", "WeekDay", "Hour"],
    )

    # Excel ä¸­é¿å…ä¸­æ–‡è¡¨å¤´ä¹±ç ï¼Œç»Ÿä¸€æ”¹ä¸ºè‹±æ–‡æ ‡é¢˜
    header = '"content","EDT_time","Beijing_time","year","Month","WeekDay","Hour"'

    with open(out_name, "w", encoding="utf-8", newline="") as out:
        out.write(header + "\n")
        for _tw_id, content, edt_str, bj_time, year_val, month_val, weekday_val, hour_val in rows:
            esc = lambda x: '"' + x.replace('"', '""') + '"'
            out.write(
                f'{esc(content)},{esc(edt_str)},{esc(bj_time)},'
                f'{year_val},{month_val},"{weekday_val}",{hour_val}\n'
            )

    # ç”Ÿæˆç»Ÿè®¡ç”¨ Excelï¼ˆå•æ–‡ä»¶å« detail + æ±‡æ€»ï¼‰
    day_stats = build_day_bucket_stats(df_clean)
    day_natural_stats = build_natural_day_stats(df_clean)
    hour_stats = build_hourly_stats(df_clean)
    excel_path = out_name.replace(".csv", "_stats.xlsx")
    with pd.ExcelWriter(excel_path, engine="xlsxwriter") as writer:
        df_clean.to_excel(writer, sheet_name="detail", index=False)
        day_stats.to_excel(writer, sheet_name="day_summary_12PM-12PM_EST", index=False)
        day_natural_stats.to_excel(writer, sheet_name="day_summary_natural_EST", index=False)
        hour_stats.to_excel(writer, sheet_name="hour_summary", index=False)

    cleaned = len(rows)
    removed = max(raw_valid - cleaned, 0)

    console.print(Panel.fit(f"""
ğŸ“¦ åŸå§‹æ–‡ä»¶ï¼š{os.path.basename(INPUT_FILE)}ï¼ˆæ€»è¡Œæ•° {total_lines}ï¼Œç–‘ä¼¼æœ‰æ•ˆè®°å½• {raw_valid}ï¼‰
ğŸ§¹ æ¸…æ´—åï¼š{os.path.basename(out_name)}ï¼ˆæœ‰æ•ˆæ¨æ–‡ {cleaned} æ¡ï¼‰
ğŸ—‘ï¸ ä¼°ç®—æ¸…ç†æ‰ç¢è¡Œ/æ— æ•ˆï¼š{removed} æ¡
ğŸ“… å…±æ£€æµ‹åˆ° {len(all_years)} å¹´ï¼š{(min(all_years) if all_years else 'â€”')} â€“ {(max(all_years) if all_years else 'â€”')}
âœ… [bold green]æ¸…æ´—å®Œæˆï¼ˆå·²å«åŒ—äº¬æ—¶é—´ + å¹´ä»½æ¨æ–­ï¼‰[/bold green]
""", title="æ¸…æ´—æŠ¥å‘Š", border_style="green"))


# ------------------------ 2) åŸºç¡€åˆ†æ ------------------------
def basic_overview(selected_file: str):
    """
    å±•ç¤ºå•ä¸ª clean CSV çš„æ ¸å¿ƒæ¦‚è§ˆï¼ŒåŒ…æ‹¬æœˆä»½/æ˜ŸæœŸ/å°æ—¶åˆ†å¸ƒä»¥åŠåŸåˆ› vs è½¬æ¨å æ¯”ã€‚
    è§£æé€»è¾‘ï¼š
      - å°†â€œåŒ—äº¬æ—¶é—´â€åˆ—è½¬æ¢ä¸º pandas æ—¶é—´æˆ³ï¼ˆå¸¦æ—¶åŒºï¼‰ï¼›
      - é€ä¸ªç»´åº¦ç»Ÿè®¡é¢‘æ¬¡å¹¶ç”¨ rich è¡¨æ ¼æ¸²æŸ“ï¼›
      - è¾“å‡ºè¡¥å……æŒ‡æ ‡ï¼ˆå¦‚è½¬æ¨å æ¯”ï¼‰ã€‚
    """
    console.print(f"\n[bold]ğŸ“Š æ­£åœ¨åˆ†æï¼š[/bold]{selected_file}\n")
    df = pd.read_csv(selected_file)

    # â€”â€” ç»Ÿä¸€æŠŠâ€œCSTâ€å½“ä½œä¸­å›½æ ‡å‡†æ—¶é—´ â€”â€” #
    BJ_TZ = "Asia/Shanghai"

    def parse_bj_time(s: str):
        """
        æœŸæœ›è¾“å…¥ï¼š'YYYY-MM-DD HH:MM:SS CST' æˆ– 'YYYY-MM-DD HH:MM:SS'
        æ— è®ºæœ‰æ²¡æœ‰ 'CST'ï¼Œéƒ½æŒ‰åŒ—äº¬æ—¶é—´æœ¬åœ°åŒ–
        """
        if pd.isna(s):
            return pd.NaT
        txt = str(s).strip()
        if not txt:
            return pd.NaT

        # å»æ‰å¯èƒ½å‡ºç°çš„ 'CST'ï¼ˆé˜²æ­¢è¢«è¯¯åˆ¤ä¸ºç¾ä¸­ CSTï¼‰
        txt = txt.replace("CST", "").strip()

        # æ˜ç¡®ç”¨å›ºå®šæ ¼å¼è§£æï¼Œé¿å… pandas çŒœæµ‹
        # ä½ çš„æ¸…æ´—ä»£ç é‡ŒåŒ—äº¬æ—¶é—´å°±æ˜¯ 'YYYY-MM-DD HH:MM:SS'
        try:
            dt = datetime.strptime(txt, "%Y-%m-%d %H:%M:%S")
            # æœ¬åœ°åŒ–åˆ°åŒ—äº¬æ—¶åŒº
            return pd.Timestamp(dt, tz=BJ_TZ)
        except Exception:
            # å…œåº•ï¼šæœ€åå†è®© pandas å°è¯•è§£æï¼Œä½†ç«‹åˆ»æœ¬åœ°åŒ–/è½¬æ¢åˆ°åŒ—äº¬æ—¶åŒº
            ts = pd.to_datetime(txt, errors="coerce", utc=True)
            if pd.isna(ts):
                return pd.NaT
            return ts.tz_convert(BJ_TZ)

    # âœ… ä½¿ç”¨ç¨³å¥è§£æ
    df["datetime_bj"] = df["åŒ—äº¬æ—¶é—´"].apply(parse_bj_time)

    # å¦‚æœåç»­éœ€è¦æ— æ—¶åŒºçš„â€œæ™®é€šæ—¶é—´åˆ—â€ï¼Œå¯ä»¥å†åŠ ï¼š
    # df["datetime_bj_naive"] = df["datetime_bj"].dt.tz_convert(BJ_TZ).dt.tz_localize(None)

    # å†…å®¹åˆ—ï¼ˆè‡ªåŠ¨å–ç¬¬ 2 åˆ—åï¼‰
    content_col = df.columns[1]

    # æœˆä»½ç»Ÿè®¡
    console.print(Panel.fit("ğŸ“† æŒ‰æœˆä»½ç»Ÿè®¡å‘æ¨æ•°é‡", border_style="cyan"))
    if df["datetime_bj"].notna().any():
        df["month_num"] = df["datetime_bj"].dt.month
        month_counts = df["month_num"].value_counts().sort_index()
        if month_counts.empty:
            console.print("[yellow]âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°æœˆä»½æ•°æ®[/yellow]")
        else:
            t = Table(show_header=True, header_style="bold blue")
            t.add_column("æœˆä»½")
            t.add_column("å‘æ¨æ•°é‡", justify="right")
            for m, c in month_counts.items():
                t.add_row(calendar.month_abbr[m], str(int(c)))
            console.print(t)
    else:
        console.print("[yellow]âš ï¸ æ— æ³•è§£æåŒ—äº¬æ—¶é—´ï¼Œæœˆä»½è¡¨ä¸ºç©º[/yellow]")

    # æ˜ŸæœŸç»Ÿè®¡ï¼ˆæŒ‰æœˆä»½æ±‡æ€»ï¼‰
    console.print(Panel.fit("ğŸ“… å‘¨ä¸€è‡³å‘¨æ—¥å‘æ¨æ•°é‡ï¼ˆæŒ‰æœˆä»½æ±‡æ€»ï¼‰", border_style="magenta"))
    if df["datetime_bj"].notna().any():
        df["weekday"] = df["datetime_bj"].dt.day_name()
        df["month_num"] = df["datetime_bj"].dt.month
        pivot = pd.pivot_table(df, index="month_num", columns="weekday", values=content_col, aggfunc="count", fill_value=0)
        if not pivot.empty:
            pivot = pivot.reindex(columns=WEEK_ORDER).fillna(0)
            t = Table(show_header=True, header_style="bold magenta")
            t.add_column("æœˆä»½")
            for w in WEEK_ORDER:
                t.add_column(w, justify="right")
            for m in pivot.index:
                row = [calendar.month_abbr[m]] + [str(int(v)) for v in pivot.loc[m, WEEK_ORDER]]
                t.add_row(*row)
            console.print(t)
        else:
            console.print("[yellow]âš ï¸ æ— æ³•ç”Ÿæˆæ˜ŸæœŸåˆ†å¸ƒè¡¨[/yellow]")
    else:
        console.print("[yellow]âš ï¸ æ— æ³•è§£æåŒ—äº¬æ—¶é—´ï¼Œæ˜ŸæœŸè¡¨ä¸ºç©º[/yellow]")

    # å°æ—¶åˆ†å¸ƒï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
    console.print(Panel.fit("ğŸ•’ æ¯å°æ—¶ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰å‘æ¨æ•°é‡", border_style="green"))
    if df["datetime_bj"].notna().any():
        df["hour"] = df["datetime_bj"].dt.hour
        hour_counts = df["hour"].value_counts().sort_index()
        if hour_counts.empty:
            console.print("[yellow]âš ï¸ æ— å°æ—¶åˆ†å¸ƒæ•°æ®[/yellow]")
        else:
            t = Table(show_header=True, header_style="bold yellow")
            t.add_column("åŒ—äº¬æ—¶é—´")
            t.add_column("å‘æ¨æ•°é‡", justify="right")
            for h, c in hour_counts.items():
                t.add_row(f"{h:02d}:00â€“{(h+1)%24:02d}:00", str(int(c)))
            console.print(t)
    else:
        console.print("[yellow]âš ï¸ æ— æ³•è§£æåŒ—äº¬æ—¶é—´ï¼Œå°æ—¶è¡¨ä¸ºç©º[/yellow]")

    # åŸåˆ›/è½¬æ¨å æ¯”
    df["is_rt"] = df[content_col].astype(str).str.startswith("RT @")
    rt_ratio = float(df["is_rt"].mean()) if len(df) else 0.0
    console.print(f"\nğŸ’¬ è½¬æ¨å æ¯”ï¼š{rt_ratio:.2%}ï¼ŒåŸåˆ›å æ¯”ï¼š{1-rt_ratio:.2%}")


# ------------------------ 3) é«˜çº§è‡ªå®šä¹‰åˆ†æ ------------------------
from rich.style import Style
from rich.box import SIMPLE_HEAD

def advanced_analysis(selected_file: str):
    """
    æ ¹æ®ç”¨æˆ·é€‰æ‹©æ‰§è¡Œ N ä¸ªæœˆæ»šåŠ¨çª—å£çš„å‘¨ç»´åº¦ / å°æ—¶ç»´åº¦ / å‘¨Ã—å°æ—¶çƒ­åŠ›åˆ†æã€‚
    é‡ç‚¹ï¼šå…ˆç­›é€‰æŒ‡å®šèŒƒå›´çš„æ•°æ®ï¼Œå†æ„å»ºæ•°æ®é€è§†è¡¨å¹¶ä»¥é¢œè‰²ç¼–ç å±•ç¤ºå¯†åº¦ã€‚
    """
    console.print(Panel.fit("ğŸ§­ é«˜çº§è‡ªå®šä¹‰åˆ†æ\n1) è¿‡å» N ä¸ªæœˆï¼šå‘¨ä¸€è‡³å‘¨æ—¥åˆ†å¸ƒ\n2) è¿‡å» N ä¸ªæœˆï¼šå°æ—¶åˆ†å¸ƒ\n3) è¿‡å» N ä¸ªæœˆï¼šå‘¨Ã—å°æ—¶äºŒç»´åˆ†å¸ƒï¼ˆå½©è‰²çƒ­åº¦ï¼‰\n4) è¿”å›ä¸Šä¸€çº§",
                            border_style="cyan"))
    choice = Prompt.ask("è¯·é€‰æ‹©åŠŸèƒ½", choices=["1","2","3","4"])
    if choice == "4":
        return

    df = pd.read_csv(selected_file)
    df["datetime_bj"] = pd.to_datetime(df["åŒ—äº¬æ—¶é—´"], errors="coerce")
    if not df["datetime_bj"].notna().any():
        console.print("[red]âŒ æ— æ³•è§£æåŒ—äº¬æ—¶é—´ï¼Œæ— æ³•è¿›è¡Œé«˜çº§åˆ†æ[/red]")
        return

    months_back = IntPrompt.ask("è¯·è¾“å…¥è¿‡å» N ä¸ªæœˆï¼ˆâ‰¥1ï¼‰", default=3, show_default=True)
    if months_back < 1:
        months_back = 1

    dfw = filter_last_n_months(df, months_back)
    if dfw.empty:
        console.print("[yellow]âš ï¸ æŒ‡å®šèŒƒå›´å†…æ— æ•°æ®[/yellow]")
        return

    dfw["weekday"] = dfw["datetime_bj"].dt.day_name()
    dfw["hour"] = dfw["datetime_bj"].dt.hour
    content_col = dfw.columns[1]

    # ===== å‘¨ + å°æ—¶äºŒç»´è¡¨ =====
    if choice == "3":
        console.print(Panel.fit(f"ğŸ§© è¿‡å» {months_back} ä¸ªæœˆï¼šå‘¨Ã—å°æ—¶äºŒç»´åˆ†å¸ƒï¼ˆåŒ—äº¬æ—¶é—´ï¼‰", border_style="cyan"))
        pivot = pd.pivot_table(dfw, index="weekday", columns="hour", values=content_col, aggfunc="count", fill_value=0)
        pivot = pivot.reindex(index=WEEK_ORDER, columns=sorted(pivot.columns))

        # æ¸²æŸ“åˆ†ä¸¤é¡µæ˜¾ç¤º (0â€“11, 12â€“23)
        for block_start in [0, 12]:
            cols = list(range(block_start, block_start + 12))
            sub = pivot[cols].copy()

            t = Table(show_header=True, box=SIMPLE_HEAD, header_style="bold cyan", title=f"{block_start:02d}:00â€“{(block_start+11)%24:02d}:59 åŒºé—´")
            t.add_column("æ˜ŸæœŸ", justify="center", style="bold")
            for h in cols:
                t.add_column(f"{h:02d}", justify="right", width=4)

            max_val = int(sub.values.max()) if sub.values.size > 0 else 0
            for w in WEEK_ORDER:
                row = [w]
                for h in cols:
                    val = int(sub.loc[w, h]) if (w in sub.index and h in sub.columns) else 0
                    if max_val == 0:
                        color = "grey39"
                    else:
                        ratio = val / max_val
                        if ratio > 0.75:
                            color = "bold red"
                        elif ratio > 0.4:
                            color = "yellow"
                        elif ratio > 0.1:
                            color = "cyan"
                        else:
                            color = "grey39"
                    row.append(f"[{color}]{val}[/]")
                t.add_row(*row)

            console.print(t)
        return

    # ===== å‘¨ç»´åº¦ =====
    if choice == "1":
        console.print(Panel.fit(f"ğŸ“… è¿‡å» {months_back} ä¸ªæœˆï¼šå‘¨ä¸€è‡³å‘¨æ—¥å‘æ¨æ•°é‡", border_style="magenta"))
        wk_counts = dfw["weekday"].value_counts()
        t = Table(show_header=True, box=SIMPLE_HEAD, header_style="bold magenta")
        t.add_column("æ˜ŸæœŸ", justify="center")
        t.add_column("å‘æ¨æ•°é‡", justify="right")
        for w in WEEK_ORDER:
            t.add_row(w, str(int(wk_counts.get(w, 0))))
        console.print(t)
        return

    # ===== å°æ—¶ç»´åº¦ =====
    if choice == "2":
        console.print(Panel.fit(f"ğŸ•’ è¿‡å» {months_back} ä¸ªæœˆï¼šæ¯å°æ—¶ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰å‘æ¨æ•°é‡", border_style="green"))
        hr_counts = dfw["hour"].value_counts().sort_index()
        t = Table(show_header=True, box=SIMPLE_HEAD, header_style="bold yellow")
        t.add_column("åŒ—äº¬æ—¶é—´")
        t.add_column("å‘æ¨æ•°é‡", justify="right")
        max_v = int(hr_counts.max()) if not hr_counts.empty else 0
        for h in range(24):
            val = int(hr_counts.get(h, 0))
            ratio = val / max_v if max_v else 0
            if ratio > 0.75:
                color = "bold red"
            elif ratio > 0.4:
                color = "yellow"
            elif ratio > 0.1:
                color = "cyan"
            else:
                color = "grey39"
            t.add_row(f"{h:02d}:00â€“{(h+1)%24:02d}:00", f"[{color}]{val}[/]")
        console.print(t)



# ------------------------ ä¸»èœå• ------------------------
def list_clean_files() -> List[str]:
    """åˆ—å‡ºæ‰€æœ‰æ¸…æ´—è¾“å‡ºæ–‡ä»¶ï¼ˆæŒ‰æ—¶é—´å€’åºä½¿ç”¨æ–¹ä¾¿çš„æœ€æ–°ä¼˜å…ˆï¼‰ã€‚"""
    return sorted(glob.glob(f"{OUTPUT_PREFIX}_*.csv"), reverse=True)


def run_analysis_menu():
    """
    å±•ç¤º clean æ–‡ä»¶åˆ—è¡¨å¹¶è¿›å…¥åˆ†æå­èœå•ï¼Œå…è®¸ç”¨æˆ·é€‰æ‹©åŸºç¡€æ¦‚è§ˆæˆ–é«˜çº§åˆ†æã€‚
    ä½¿ç”¨å¾ªç¯ä»¥ä¾¿åœ¨å®Œæˆä¸€æ¬¡åˆ†æåç»§ç»­æ“ä½œï¼Œç›´åˆ°ç”¨æˆ·è¿”å›ä¸»èœå•ã€‚
    """
    files = list_clean_files()
    if not files:
        console.print("[red]æœªæ‰¾åˆ°æ¸…æ´—åçš„æ–‡ä»¶ï¼Œè¯·å…ˆæ‰§è¡Œã€1 æ•°æ®æ¸…æ´—ã€‘ï¼[/red]")
        return

    console.print("\nğŸ“‚ å¯ç”¨æ¸…æ´—æ–‡ä»¶ï¼ˆæ–°â†’æ—§ï¼‰ï¼š")
    tb = Table(show_header=True, header_style="bold magenta")
    tb.add_column("ç¼–å·", justify="center", width=6)
    tb.add_column("æ–‡ä»¶å", overflow="fold")
    tb.add_column("æœ€åä¿®æ”¹æ—¶é—´", justify="right")
    for i, f in enumerate(files, 1):
        mtime = datetime.fromtimestamp(os.path.getmtime(f)).strftime("%Y-%m-%d %H:%M:%S")
        tb.add_row(str(i), os.path.basename(f), mtime)
    console.print(tb)

    idx = Prompt.ask("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡ä»¶ç¼–å·", choices=[str(i) for i in range(1, len(files)+1)])
    selected_file = files[int(idx)-1]

    while True:
        console.print(Panel.fit("ğŸ“Š åˆ†æåŠŸèƒ½\n1) åŸºç¡€æ¦‚è§ˆ\n2) é«˜çº§è‡ªå®šä¹‰åˆ†æï¼ˆN ä¸ªæœˆï¼šå‘¨/å°æ—¶/å‘¨Ã—å°æ—¶ï¼‰\n3) è¿”å›ä¸»èœå•", border_style="blue"))
        c = Prompt.ask("è¯·é€‰æ‹©", choices=["1","2","3"])
        if c == "1":
            for _ in track(range(2), description="ğŸ” ç”Ÿæˆæ¦‚è§ˆâ€¦"):
                pass
            basic_overview(selected_file)
        elif c == "2":
            advanced_analysis(selected_file)
        else:
            break


def main_menu():
    """
    é¡¶å±‚å…¥å£ï¼šæä¾›â€œæ¸…æ´—/åˆ†æ/é€€å‡ºâ€ä¸‰ä¸ªé€‰é¡¹ï¼Œå¹¶è°ƒç”¨å¯¹åº”æµç¨‹ã€‚
    é€šè¿‡è¿›åº¦æ¡æç¤ºå…³é”®æ­¥éª¤çš„æ‰§è¡Œæ„Ÿï¼Œæå‡ CLI ä½“éªŒã€‚
    """
    while True:
        console.print(Panel.fit("[bold cyan]Elon Musk æ¨æ–‡æ•°æ®å·¥å…·[/bold cyan]\n1ï¸âƒ£ æ•°æ®æ¸…æ´—\n2ï¸âƒ£ æ•°æ®åˆ†æ\n3ï¸âƒ£ é€€å‡º", border_style="cyan"))
        choice = Prompt.ask("è¯·é€‰æ‹©æ“ä½œ", choices=["1","2","3"])
        if choice == "1":
            for _ in track(range(3), description="ğŸ§¹ æ­£åœ¨æ¸…æ´—â€¦"):
                pass
            run_cleaning()
        elif choice == "2":
            run_analysis_menu()
        else:
            console.print("[yellow]ğŸ‘‹ å†è§ï¼[/yellow]")
            break


if __name__ == "__main__":
    main_menu()
